# -*- coding: utf-8 -*-
"""MedGemma_fine_tuning_intro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UG-LcewH3mvUOaaNzex2kGLW96j8Ye5k
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip uninstall -y tensorflow tensorflow-text tensorflow-datasets keras tensorflow-macos tensorflow-metal

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U "transformers>=4.50.0" accelerate torch torchvision

from huggingface_hub import login

login()

import os
# Optional but keeps TF out of the way
os.environ["TRANSFORMERS_NO_TF"] = "1"
os.environ["TRANSFORMERS_NO_FLAX"] = "1"

import torch

from transformers import AutoProcessor, AutoModelForImageTextToText

model_id = "google/medgemma-4b-it"

model = AutoModelForImageTextToText.from_pretrained(
    model_id,
    dtype=torch.bfloat16,      # `dtype` is the new name for `torch_dtype`
    device_map="auto",
)
processor = AutoProcessor.from_pretrained(model_id)

print(model.__class__)

print(model)

# does not computer gradients for every parameter in vision tower
for p in model.vision_tower.parameters():
    p.requires_grad = False

# should give 0 for all parameters being frozen (False is treated as 0)
sum(p.requires_grad for p in model.vision_tower.parameters())

from peft import LoraConfig, get_peft_model

# defines LoRA behavior
lora_config = LoraConfig( # create config object to describe LoRA setup
    r=16, # rank of low-rank matrix
    lora_alpha=32, # scaling factor, generally 2 * r
    target_modules=["q_proj", "v_proj"], # attach LoRA adapters to these layersâ€”query and value projection
    lora_dropout=0.05, # small dropout
    bias="none", # no addition or modification of biases
)

model = get_peft_model(model, lora_config) # get LoRA-augmented version of model

model.print_trainable_parameters()
# only 6.4 mil out of 4.3 bil params are trainable
# only ~0.15% of params are trainable