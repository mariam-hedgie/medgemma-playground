# -*- coding: utf-8 -*-
"""MedGemma_hf_example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-A9g_fHTjn0uhjvOO9ZWumbreraY6xCw
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/Google-Health/medgemma.git
# %cd medgemma
!ls

!ls notebooks

!pip install -U "transformers>=4.50.0" accelerate pillow requests

import torch
print("Torch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

from huggingface_hub import login

login()

# Import the Hugging Face pipeline function (high-level API for inference)
from transformers import pipeline

# PIL = Python Imaging Library, used to open / manipulate images
from PIL import Image

# Library for downloading files from the internet (we use it to fetch the X-ray)
import requests

# PyTorch — used for controlling GPU, dtype (precision), tensors, and running the model
import torch


# -----------------------------------------------------------
# LOAD THE MODEL PIPELINE
# -----------------------------------------------------------

pipe = pipeline(
    "image-text-to-text",        # Specify the type of model: takes image + text, outputs text
    model="google/medgemma-4b-it",  # The exact model ID on Hugging Face
    torch_dtype=torch.bfloat16,     # Use BF16 precision for better performance on GPU
    device="cuda",                  # Run the model on GPU ('cuda') instead of CPU
)


# -----------------------------------------------------------
# LOAD THE X-RAY IMAGE
# -----------------------------------------------------------

# URL of a public chest X-ray image
image_url = "https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png"

# Download the image and open it with PIL
# - requests.get() pulls the raw bytes
# - .raw gives a file-like object
# - Image.open() converts it into a PIL Image object
image = Image.open(
    requests.get(image_url, headers={"User-Agent": "example"}, stream=True).raw
)


# -----------------------------------------------------------
# PREPARE THE CHAT-STYLE INPUT FOR MEDGEMMA
# -----------------------------------------------------------

# MedGemma uses a structured chat format:
# - "role": system or user
# - "content": list of text and/or image elements

messages = [
    {
        "role": "system",
        "content": [
            # System content: instructs the model how to behave
            {"type": "text", "text": "You are an expert radiologist."}
        ]
    },
    {
        "role": "user",
        "content": [
            # User instruction
            {"type": "text", "text": "Describe this X-ray"},
            # Attach the actual X-ray image to the prompt
            {"type": "image", "image": image}
        ]
    }
]


# -----------------------------------------------------------
# RUN THE MODEL (INFERENCE)
# -----------------------------------------------------------

# pipe(...) runs the whole process:
# - preprocess text + image
# - forward pass through the model
# - generate the radiology-style output text
output = pipe(
    text=messages,
    max_new_tokens=200   # limit the size of generated text to avoid overly long outputs
)


# -----------------------------------------------------------
# PRINT THE MODEL'S OUTPUT
# -----------------------------------------------------------

# output is a list → we take output[0]
# "generated_text" is a list of chat messages
# [-1] is the latest (assistant) message
# ["content"] is the actual generated text
print(output[0]["generated_text"][-1]["content"])

from IPython.display import display

# Image attribution: Stillwaterising, CC0, via Wikimedia Commons
image_url = "https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png"
image = Image.open(requests.get(image_url, headers={"User-Agent": "example"}, stream=True).raw)

display(image)  # this will show the X-ray in the output cell